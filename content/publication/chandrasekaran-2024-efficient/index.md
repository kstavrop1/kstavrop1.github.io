---
title: Efficient Discrepancy Testing for Learning with Distribution Shift
date: '2024-06-06'
draft: false
publishDate: '2024-08-12T19:47:11.824636Z'
authors:
- Gautam Chandrasekaran
- Adam Klivans
- Vasilis Kontonis
- admin
- Arsen Vasilyan
publication_types:
- 'atricle'
abstract: 'A fundamental notion of distance between train and test distributions from the field of domain adaptation is discrepancy distance. While in general hard to compute, here we provide the first set of provably efficient algorithms for testing localized discrepancy distance, where discrepancy is computed with respect to a fixed output classifier. These results imply a broad set of new, efficient learning algorithms in the recently introduced model of Testable Learning with Distribution Shift (TDS learning) due to Klivans et al. (2023). 

Our approach generalizes and improves all prior work on TDS learning: (1) we obtain universal learners that succeed simultaneously for large classes of test distributions, (2) achieve near-optimal error rates, and (3) give exponential improvements for constant depth circuits. Our methods further extend to semi-parametric settings and imply the first positive results for low-dimensional convex sets. Additionally, we separate learning and testing phases and obtain algorithms that run in fully polynomial time at test time.'
featured: false
publication: '*arXiv preprint arXiv:2406.09373*'
url_pdf: https://arxiv.org/pdf/2406.09373
links:
- name: URL
  url: https://arxiv.org/abs/2406.09373
---

